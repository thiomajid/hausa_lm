# VAE Training Configuration
# Generic configuration that works with any HuggingFace dataset

defaults:
  - model: vae
  - trainer: base
  - _self_

# Dataset configuration
# Examples:
# - "ylecun/mnist" (28x28 grayscale)
# - "fashion_mnist" (28x28 grayscale)
# - "uoft-cs/cifar10" (32x32 RGB)
# - "imagenet-1k" (224x224 RGB)
dataset:
  name: "ylecun/mnist" # HuggingFace dataset name
  image_key: null # Auto-detect image key (or specify: "image", "img", "pixel_values")
  batch_size: 128
  shuffle_buffer_size: 10000
  num_workers: 4
  worker_buffer_size: 100

# Training configuration
training:
  num_epochs: 20
  learning_rate: 1e-3
  beta: 1.0 # Beta for beta-VAE (KL weight)
  warmup_steps: 1000
  weight_decay: 1e-4
  gradient_clip_norm: 1.0

# Optimizer configuration
optimizer:
  name: "adam"
  betas: [0.9, 0.999]
  eps: 1e-8

# Logging and checkpointing
logging:
  log_every: 100
  eval_every: 1000
  checkpoint_every: 5000
  num_samples_to_generate: 16 # For PNG visualization
  tensorboard_samples: 8 # For TensorBoard logging

# Paths
output_dir: "./outputs/vae"
checkpoint_dir: "./checkpoints/vae"

# Reproducibility
seed: 42

# Hardware
device: "auto" # auto, cpu, gpu
