text_config:
  vocab_size: 49_152
  context_length: 256
  num_blocks: 20
  slstm_at: [0, 2, 4, 6, 8, 12, 14, 16, 18]
  embedding_dim: 576
  mlstm_block:
    mlstm:
      conv1d_kernel_size: 4
      qkv_proj_blocksize: 32
      num_heads: 4
  slstm_block:
    slstm:
      backend: "vanilla"
      num_heads: 4
      conv1d_kernel_size: 4
      bias_init: "powerlaw_blockdependent"
    feedforward:
      proj_factor: 1.7
      act_fn: "swish"
